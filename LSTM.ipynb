{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "5da06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁sa', 'mp', 'le', '▁sentence', '▁to', '▁to', 'ken', 'ize', '.']\n",
      "[102, 22, 14, 1693, 2265, 421, 7627, 6, 6, 4296, 3055, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from numpy import longdouble\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "text = \"This is a sample sentence to tokenize.\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens)\n",
    "print(sp.encode_as_ids(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "783e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sequence_length():\n",
    "    maxlen = 0\n",
    "    prefix = './PhishingEmails/'  # Adjust this to your file path\n",
    "    \n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read()\n",
    "            email_dict = json.loads(jsonStr)\n",
    "            setupData = sp.encode_as_ids(\n",
    "                email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip()\n",
    "            )\n",
    "\n",
    "            if(len(setupData) < 725):\n",
    "                maxlen = max(maxlen, len(setupData))\n",
    "\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "09af4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1512.80it/s]\n",
      "  0%|          | 0/3332 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[434], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mload_emails\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[431], line 26\u001b[0m, in \u001b[0;36mload_emails\u001b[1;34m(batch_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m     setupData \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m pad_length\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_counter \u001b[38;5;241m<\u001b[39m batch_size:\n\u001b[1;32m---> 26\u001b[0m     setupData \u001b[38;5;241m=\u001b[39m \u001b[43msetupData\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetupData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     27\u001b[0m     batch\u001b[38;5;241m.\u001b[39mappend(setupData)\n\u001b[0;32m     28\u001b[0m     batch_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "load_emails(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb41a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(batch_size):\n",
    "\n",
    "    max_sequence_length = find_max_sequence_length()\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "    ctr = 0\n",
    "    \n",
    "    prefix = './PhishingEmails/' #change this to the prefile thing such as './celebA'\n",
    "\n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        \n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read() #json file as a string\n",
    "            email_dict = json.loads(jsonStr) #converts to dictionary\n",
    "        \n",
    "        setupData = sp.encode_as_ids(email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip())\n",
    "        pad_length = max_sequence_length - len(setupData)\n",
    "\n",
    "        if(len(setupData) <= max_sequence_length):\n",
    "            ctr+=1\n",
    "            if pad_length > 0:\n",
    "                setupData += [-1] * pad_length\n",
    "\n",
    "            if batch_counter < batch_size:\n",
    "                # setupData = setupData[np.isfinite(setupData)]\n",
    "                batch.append(setupData)\n",
    "                batch_counter += 1\n",
    "            else:\n",
    "                dataset.append(batch)\n",
    "                batch = []\n",
    "                batch_counter = 0\n",
    "    toRet = np.array(dataset, dtype=longdouble)\n",
    "    toRet = toRet[~np.isnan(toRet).any(axis=1)]\n",
    "    return toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize helpful functions for math\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "def tanh_derivative(x:np.ndarray):\n",
    "    return 1-np.square(tanh(x))\n",
    "\n",
    "def softmax(x: np.ndarray):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy(yhat, y, epsilon=1e-10):\n",
    "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)  # Clip yhat to avoid zeros\n",
    "    return -np.sum(y * np.log(yhat_clipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes the weights of the network\n",
    "def initialize_cell(input_size, hidden_size):\n",
    "\n",
    "    print(input_size)\n",
    "\n",
    "    cell = {}\n",
    "\n",
    "    cell[\"W_i\"] = np.hstack((np.random.normal(0, 0.01, (hidden_size, hidden_size)), np.random.normal(0, 0.01, (hidden_size, input_size)))) #input gate weights\n",
    "    cell[\"W_f\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #forget gate weights\n",
    "    cell[\"W_c\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #candidate gate weights\n",
    "    cell[\"W_o\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #output gate weights\n",
    "    cell[\"W_y\"] = (np.random.normal(0,1,(hidden_size,hidden_size)))#final gate weights\n",
    "\n",
    "    #not sure if the biases need to be 3d...\n",
    "    cell[\"b_i\"] = np.zeros(hidden_size) #input gate biases\n",
    "    cell[\"b_f\"] = np.zeros(hidden_size) #forget gate biases\n",
    "    cell[\"b_c\"] = np.zeros(hidden_size) #candidate gate biases\n",
    "    cell[\"b_o\"] = np.zeros(hidden_size) #output gate biases\n",
    "    cell[\"b_y\"] = np.zeros(hidden_size) #final gate biases\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass of all gates\n",
    "def forward_pass(cell, prevA, prevC, X):\n",
    "\n",
    "    # print(X, \"/n______-\")\n",
    "    \n",
    "    input = np.hstack((prevA, X))\n",
    "\n",
    "    forward = {}\n",
    "\n",
    "    forward[\"F\"] = sigmoid(cell[\"W_f\"].dot(input.T) + cell[\"b_i\"])\n",
    "    \n",
    "    forward[\"C\"] = tanh(cell[\"W_c\"].dot(input.T) + cell[\"b_c\"])\n",
    "\n",
    "    forward[\"I\"] = sigmoid(cell[\"W_i\"].dot(input.T) + cell[\"b_i\"])\n",
    "\n",
    "    forward[\"O\"] = sigmoid(cell[\"W_o\"].dot(input.T) + cell[\"b_o\"])\n",
    "\n",
    "\n",
    "    forward[\"prevA\"] = prevA\n",
    "    forward[\"prevC\"] = prevC\n",
    "    forward[\"C_t\"] = (forward[\"prevC\"] * forward[\"F\"]) + (forward[\"I\"] * forward[\"C\"])\n",
    "    forward[\"A_t\"] = forward[\"O\"] * tanh(forward[\"C_t\"])\n",
    "\n",
    "    forward[\"Z_t\"] = cell[\"W_y\"].dot(forward[\"C_t\"] * forward[\"O\"]) + cell[\"b_y\"]\n",
    "    # print(forward[\"Z_t\"], \"  Z\")\n",
    "    forward[\"Yhat\"] = softmax(forward[\"Z_t\"])\n",
    "\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(forward, cell, X, Y, lprimea, lprimec):\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # print(\"BackProp\")\n",
    "    input = np.hstack((forward[\"prevA\"], X))\n",
    "\n",
    "    dldA_t = np.transpose(cell[\"W_y\"]).dot(forward[\"Yhat\"]-Y) + lprimea\n",
    "    dldC_t = lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"])) * dldA_t \n",
    "\n",
    "    TdLdw_f = (dldC_t * forward[\"prevC\"] * forward[\"F\"]*(1-forward[\"F\"])) \n",
    "    TdLdw_c = (dldC_t * forward[\"I\"])\n",
    "    TdLdw_o = (dldA_t * tanh(forward[\"C_t\"]) * forward[\"O\"] * (1-forward[\"O\"]))\n",
    "    TdLdw_i = (dldC_t * forward[\"C\"] * forward[\"I\"] * (1-forward[\"I\"]))\n",
    "\n",
    "    # np.atleast2d(a).T\n",
    "\n",
    "    woa = cell[\"W_o\"][:, :128]\n",
    "    wca = cell[\"W_c\"][:, :128]\n",
    "    wia = cell[\"W_i\"][:, :128]\n",
    "    wfa = cell[\"W_f\"][:, :128]\n",
    "\n",
    "\n",
    "    grads[\"dLda_prev\"] = woa.T.dot(TdLdw_o) + wca.T.dot(TdLdw_c) + wia.T.dot(TdLdw_i) + wfa.T.dot(TdLdw_f)\n",
    "    grads[\"dLdc_prev\"] = (lprimec + forward[\"O\"] * 1-np.square(tanh(forward[\"C_t\"])) * dldA_t) * forward[\"F\"]\n",
    "\n",
    "\n",
    "    #not sure which side to transpose.\n",
    "    grads[\"dLdw_f\"] = np.atleast_2d(TdLdw_f).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_c\"] = np.atleast_2d(TdLdw_c).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_o\"] = np.atleast_2d(TdLdw_o).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_i\"] = np.atleast_2d(TdLdw_i).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_y\"] = (forward[\"Yhat\"] - Y).T.dot(np.transpose(forward[\"A_t\"]))\n",
    "\n",
    "    grads[\"dLdb_f\"] = 1\n",
    "    grads[\"dLdb_c\"] = 1\n",
    "    grads[\"dLdb_o\"] = 1\n",
    "    grads[\"dLdb_i\"] = 1\n",
    "    grads[\"dLdb_y\"] = 1\n",
    "\n",
    "\n",
    "    \n",
    "    loss = cross_entropy(forward[\"Yhat\"], Y)\n",
    "    print(loss)\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(cell, X, input_size, hidden_size, lr, batch_size):\n",
    "   \n",
    "    \n",
    "\n",
    "    # for b in range(0, batch_size):\n",
    "\n",
    "    prevA = np.zeros(hidden_size)\n",
    "    prevC = np.zeros(hidden_size)\n",
    "\n",
    "    gradientTot = {}\n",
    "    lossTot = 0\n",
    "\n",
    "    allForwards = []\n",
    "    labels = []\n",
    "\n",
    "    lprimea = np.zeros(hidden_size)\n",
    "    lprimec = np.zeros(hidden_size)\n",
    "\n",
    "                            # np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_f\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_c\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_o\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_i\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_y\"] = np.zeros((hidden_size,hidden_size))\n",
    "    gradientTot[\"dLdb_f\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_c\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_o\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_i\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_y\"] = np.zeros(hidden_size)\n",
    "\n",
    "    X_b = X[0]\n",
    "\n",
    "    for i in range(1, len(X_b)-1):  #tqdm will create a loading bar for your loop\n",
    "    \n",
    "        curData = X_b[0:i]\n",
    "\n",
    "        length = len(curData)\n",
    "\n",
    "        pad_length = input_size - length\n",
    "\n",
    "        if pad_length > 0:\n",
    "            curData = np.concatenate((curData,np.array([0] * pad_length)))\n",
    "        \n",
    "        forward = forward_pass(cell, prevA, prevC, curData)\n",
    "\n",
    "        allForwards.append(forward)\n",
    "\n",
    "        prevA = forward[\"A_t\"]\n",
    "        prevC = forward[\"C_t\"]\n",
    "\n",
    "        curData[length] = X_b[i+1]\n",
    "\n",
    "        labels.append(curData)\n",
    "\n",
    "\n",
    "    for i in range(0, len(allForwards)):\n",
    "      \n",
    "\n",
    "\n",
    "        grad, loss = gradient(allForwards[i], cell, labels[i], allForwards[i][\"Yhat\"], lprimea, lprimec)\n",
    "        lprimea = grad[\"dLda_prev\"]\n",
    "        lprimec = grad[\"dLdc_prev\"]\n",
    "\n",
    "        gradientTot[\"dLdw_f\"] += grad[\"dLdw_f\"]\n",
    "        gradientTot[\"dLdw_c\"] += grad[\"dLdw_c\"]\n",
    "        gradientTot[\"dLdw_o\"] += grad[\"dLdw_o\"]\n",
    "        gradientTot[\"dLdw_i\"] += grad[\"dLdw_i\"]\n",
    "        gradientTot[\"dLdw_y\"] += grad[\"dLdw_y\"]\n",
    "        \n",
    "        gradientTot[\"dLdb_f\"] += grad[\"dLdb_f\"]\n",
    "        gradientTot[\"dLdb_c\"] += grad[\"dLdb_c\"]\n",
    "        gradientTot[\"dLdb_o\"] += grad[\"dLdb_o\"]\n",
    "        gradientTot[\"dLdb_i\"] += grad[\"dLdb_i\"]\n",
    "        gradientTot[\"dLdb_y\"] += grad[\"dLdb_y\"]\n",
    "\n",
    "        # print(grad[\"dLdw_f\"], grad[\"dLdw_c\"], grad[\"dLdw_o\"], grad[\"dLdw_i\"])\n",
    "\n",
    "        lossTot += loss\n",
    "    \n",
    "\n",
    "    cell[\"W_f\"] = cell[\"W_f\"] - gradientTot[\"dLdw_f\"] * lr\n",
    "    cell[\"W_c\"] = cell[\"W_c\"] - gradientTot[\"dLdw_c\"] * lr\n",
    "    cell[\"W_o\"] = cell[\"W_o\"] - gradientTot[\"dLdw_o\"] * lr\n",
    "    cell[\"W_i\"] = cell[\"W_i\"] - gradientTot[\"dLdw_i\"] * lr\n",
    "    cell[\"W_y\"] = cell[\"W_y\"] - gradientTot[\"dLdw_y\"] * lr\n",
    "\n",
    "    cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"] * lr\n",
    "    cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"] * lr\n",
    "    cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"] * lr\n",
    "    cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"] * lr\n",
    "    cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"] * lr\n",
    "\n",
    "    return lossTot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad233117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(input_size, hidden_size, dataset, batch_size):\n",
    "\n",
    "    cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data in dataset:\n",
    "        # print(data)\n",
    "        loss = descent(cell, data, input_size, hidden_size, 0.0000001, batch_size)\n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3332 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1651.86it/s]\n",
      "100%|██████████| 3332/3332 [00:02<00:00, 1645.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "vocab_size = 724\n",
    "\n",
    "hidden_size = 128\n",
    "input_size = hidden_size + vocab_size\n",
    "\n",
    "dataset = load_emails(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "f2272f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876121606335744\n",
      "1.4331230783156856\n",
      "0.38752456947956376\n",
      "0.37423282399255664\n",
      "0.2131663227673421\n",
      "0.0008166267040099707\n",
      "0.4497566096131024\n",
      "0.789059926341296\n",
      "0.4115581851074741\n",
      "0.016417552868998785\n",
      "0.405844054846491\n",
      "0.8810087593251433\n",
      "0.07115235950728982\n",
      "0.5883263456794929\n",
      "0.36866683539906975\n",
      "0.027959955399407967\n",
      "0.0009139286299569124\n",
      "0.6463220131131662\n",
      "0.42174844207763196\n",
      "7.707939338415451e-09\n",
      "8.882065671790448e-07\n",
      "1.353630758834512e-08\n",
      "0.5806968824756492\n",
      "0.27073174663353894\n",
      "0.07352383862880611\n",
      "0.30212141784940105\n",
      "0.01965305166806664\n",
      "0.0007884166475583399\n",
      "0.01928658779216722\n",
      "1.1440148075844668e-08\n",
      "0.23898164259542992\n",
      "0.14081254101665183\n",
      "0.013285247371717067\n",
      "0.1271080638736051\n",
      "0.7757456081675826\n",
      "1.0025727927915076e-10\n",
      "0.37507155784594176\n",
      "0.8637299744597082\n",
      "1.9359015282914037e-08\n",
      "0.6386361331522326\n",
      "0.01203311738601679\n",
      "3.6081493216157306e-06\n",
      "2.7156436439857272e-05\n",
      "0.0001138014767083307\n",
      "0.00043263349625422855\n",
      "8.973828415458993e-06\n",
      "0.0008438769705098503\n",
      "0.0009053876082199869\n",
      "0.08105654256766705\n",
      "0.042639988358879\n",
      "0.5165872048782911\n",
      "1.1147612470330805e-05\n",
      "0.028373407760618675\n",
      "3.280319886665712e-08\n",
      "3.163494630076209e-07\n",
      "0.024415991494655037\n",
      "0.00013506771119678923\n",
      "7.947568295618871e-05\n",
      "7.729090314310685e-05\n",
      "0.21088543041498767\n",
      "0.01035223229015858\n",
      "0.0004959451557036168\n",
      "0.00025591408121759704\n",
      "0.06779664490762446\n",
      "0.6937269035056546\n",
      "2.608996438802822e-06\n",
      "6.095603182302212e-08\n",
      "7.012508204963676e-08\n",
      "3.827628492598607e-09\n",
      "2.464425887160671e-10\n",
      "1.3214934733292248e-10\n",
      "1.017642589917942e-10\n",
      "1.6815133494915247e-10\n",
      "3.266344564053874e-10\n",
      "1.0000000829260254e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.0000371457647944e-10\n",
      "1.0000000962582167e-10\n",
      "1.0000000827906881e-10\n",
      "1.0000000827903711e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.0016082827858822e-10\n",
      "1.1118833017346669e-10\n",
      "1.8445258306039451e-09\n",
      "0.0009988499593878044\n",
      "0.06634608862657612\n",
      "1.3419400111334454e-06\n",
      "0.6729301888162293\n",
      "0.5003620951500961\n",
      "0.5041522392852512\n",
      "0.28312221781157465\n",
      "0.33945485638654826\n",
      "0.3894766326538379\n",
      "0.47478271234352465\n",
      "0.4929234764083409\n",
      "0.5111005179204656\n",
      "0.22868516885368514\n",
      "0.10328400633640267\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "1.0047079413345764e-10\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[418], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m ex_email \u001b[38;5;241m=\u001b[39m dataset[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m finalCell \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[417], line 9\u001b[0m, in \u001b[0;36mtrain_LSTM\u001b[1;34m(input_size, hidden_size, dataset, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0000001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     11\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[413], line 44\u001b[0m, in \u001b[0;36mdescent\u001b[1;34m(cell, X, input_size, hidden_size, lr, batch_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     42\u001b[0m     curData \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((curData,np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m pad_length)))\n\u001b[1;32m---> 44\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m allForwards\u001b[38;5;241m.\u001b[39mappend(forward)\n\u001b[0;32m     48\u001b[0m prevA \u001b[38;5;241m=\u001b[39m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[411], line 10\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(cell, prevA, prevC, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((prevA, X))\n\u001b[0;32m      8\u001b[0m forward \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 10\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mcell\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tanh(cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_c\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_c\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sigmoid(cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ex_email = dataset[random.randint(0, 50)]\n",
    "\n",
    "\n",
    "\n",
    "finalCell = train_LSTM(input_size, hidden_size, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "60268a3974607f958e98a9322dd45e6d52b600b97a1d9eef60ef15acbb670d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
