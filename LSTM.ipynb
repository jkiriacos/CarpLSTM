{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "5da06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁sa', 'mp', 'le', '▁sentence', '▁to', '▁to', 'ken', 'ize', '.']\n",
      "[102, 22, 14, 1693, 2265, 421, 7627, 6, 6, 4296, 3055, 3]\n",
      "This is a sample sentence to tokenize.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from numpy import longdouble\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "text = \"This is a sample sentence to tokenize.\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens)\n",
    "print(sp.encode_as_ids(text))\n",
    "print(sp.decode(sp.encode_as_ids(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "783e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sequence_length():\n",
    "    maxlen = 0\n",
    "    prefix = './PhishingEmails/'  # Adjust this to your file path\n",
    "    \n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read()\n",
    "            email_dict = json.loads(jsonStr)\n",
    "            setupData = sp.encode_as_ids(\n",
    "                email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip()\n",
    "            )\n",
    "\n",
    "            if(len(setupData) < 725):\n",
    "                maxlen = max(maxlen, len(setupData))\n",
    "\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "edb41a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(batch_size):\n",
    "\n",
    "    max_sequence_length = find_max_sequence_length()\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "    ctr = 0\n",
    "    \n",
    "    prefix = './PhishingEmails/' #change this to the prefile thing such as './celebA'\n",
    "\n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        \n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read() #json file as a string\n",
    "            email_dict = json.loads(jsonStr) #converts to dictionary\n",
    "        \n",
    "        setupData = sp.encode_as_ids(email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip())\n",
    "        pad_length = max_sequence_length - len(setupData)\n",
    "\n",
    "        if(len(setupData) <= max_sequence_length and \"nan\" not in setupData):\n",
    "            # print(setupData,\"\\n\\n\\n\")\n",
    "            ctr+=1\n",
    "            if pad_length > 0:\n",
    "                setupData += [1] * pad_length\n",
    "\n",
    "            if batch_counter < batch_size:\n",
    "                # setupData = setupData[np.isfinite(setupData)]\n",
    "                batch.append(setupData)\n",
    "                batch_counter += 1\n",
    "            else:\n",
    "                dataset.append(batch)\n",
    "                batch = []\n",
    "                batch_counter = 0\n",
    "    toRet = np.array(dataset, dtype=longdouble)\n",
    "    print(ctr)\n",
    "    # toRet = toRet[~np.isnan(toRet).any(axis=1)]\n",
    "    np.random.shuffle(toRet)\n",
    "    # print(toRet[0])\n",
    "    return toRet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "0e301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize helpful functions for math\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "def tanh_derivative(x:np.ndarray):\n",
    "    return 1-np.square(tanh(x))\n",
    "\n",
    "def softmax(x: np.ndarray):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy(yhat, y, epsilon=1e-10):\n",
    "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)  # Clip yhat to avoid zeros\n",
    "    return -np.sum(y * np.log(yhat_clipped))\n",
    "\n",
    "def initWeights(input_size, output_size):\n",
    "    return np.random.uniform(-1, 1, (input_size, output_size)) * np.sqrt(6 / (input_size + output_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes the weights of the network\n",
    "def initialize_cell(input_size, hidden_size):\n",
    "    \n",
    "\n",
    "    cell = {}\n",
    "\n",
    "    cell[\"W_i\"] = np.hstack((initWeights(hidden_size, hidden_size), initWeights(hidden_size, input_size))) #input gate weights\n",
    "    cell[\"W_f\"] = np.hstack((initWeights(hidden_size, hidden_size), initWeights(hidden_size, input_size))) #forget gate weights\n",
    "    cell[\"W_c\"] = np.hstack((initWeights(hidden_size, hidden_size), initWeights(hidden_size, input_size))) #candidate gate weights\n",
    "    cell[\"W_o\"] = np.hstack((initWeights(hidden_size, hidden_size), initWeights(hidden_size, input_size))) #output gate weights\n",
    "    cell[\"W_y\"] = initWeights(10000, hidden_size)#final gate weights\n",
    "\n",
    "    #not sure if the biases need to be 3d...\n",
    "    cell[\"b_i\"] = np.zeros(hidden_size,dtype=longdouble) #input gate biases\n",
    "    cell[\"b_f\"] = np.zeros(hidden_size,dtype=longdouble) #forget gate biases\n",
    "    cell[\"b_c\"] = np.zeros(hidden_size,dtype=longdouble) #candidate gate biases\n",
    "    cell[\"b_o\"] = np.zeros(hidden_size,dtype=longdouble) #output gate biases\n",
    "    cell[\"b_y\"] = np.zeros(10000) #final gate biases\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass of all gates\n",
    "def forward_pass(cell, prevA, prevC, X):\n",
    "\n",
    "    # print(X, \"/n______-\")\n",
    "    \n",
    "    input = np.hstack((prevA, X))\n",
    "\n",
    "    forward = {}\n",
    "    # print(cell[\"W_f\"])\n",
    "\n",
    "    forward[\"F\"] = sigmoid(cell[\"W_f\"].dot(input) + cell[\"b_f\"])\n",
    "\n",
    "    forward[\"_c\"] = cell[\"W_c\"].dot(input) + cell[\"b_c\"]\n",
    "    \n",
    "    forward[\"C\"] = tanh(forward[\"_c\"])\n",
    "\n",
    "    forward[\"I\"] = sigmoid(cell[\"W_i\"].dot(input) + cell[\"b_i\"])\n",
    "\n",
    "    forward[\"O\"] = sigmoid(cell[\"W_o\"].dot(input) + cell[\"b_o\"])\n",
    "\n",
    "\n",
    "    forward[\"prevA\"] = prevA\n",
    "    forward[\"prevC\"] = prevC\n",
    "    forward[\"C_t\"] = (forward[\"prevC\"] * forward[\"F\"]) + (forward[\"I\"] * forward[\"C\"])\n",
    "    forward[\"A_t\"] = forward[\"O\"] * tanh(forward[\"C_t\"])\n",
    "\n",
    "    forward[\"Z_t\"] = cell[\"W_y\"].dot(forward[\"A_t\"]) \n",
    "    # + cell[\"b_y\"]\n",
    "    \n",
    "    forward[\"Yhat\"] = softmax(forward[\"Z_t\"])\n",
    "    # \n",
    "    # print(forward[\"Yhat\"].size)\n",
    "    # print(forward[\"Yhat\"], \"  Yhat\")\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(forward, cell, X, Y, lprimea, lprimec):\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # print(\"BackProp\")\n",
    "    input = np.hstack((forward[\"prevA\"], X))\n",
    "    # print((forward[\"Yhat\"]-Y).size, \"yhat-y\")\n",
    "    dldA_t = np.transpose(cell[\"W_y\"]).dot(forward[\"Yhat\"]-Y) + lprimea\n",
    "    print((forward[\"Yhat\"]))\n",
    "    dldC_t = lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"])) * dldA_t \n",
    "    # print(forward[\"Yhat\"]-Y)\n",
    "\n",
    "    TdLdw_f = (dldC_t * forward[\"prevC\"] * forward[\"F\"]*(1-forward[\"F\"])) \n",
    "    # TdLdw_c = (dldC_t * forward[\"I\"])\n",
    "    TdLdw_c = (dldC_t * forward[\"I\"]*tanh_derivative(forward[\"_c\"]))\n",
    "    TdLdw_o = (dldA_t * tanh(forward[\"C_t\"]) * forward[\"O\"] * (1-forward[\"O\"]))\n",
    "    TdLdw_i = (dldC_t * forward[\"C\"] * forward[\"I\"] * (1-forward[\"I\"]))\n",
    "    TdLdw_y = (forward[\"Yhat\"] - Y)\n",
    "\n",
    "    \n",
    "\n",
    "    # np.atleast2d(a).T\n",
    "\n",
    "    woa = cell[\"W_o\"][:, :128]\n",
    "    wca = cell[\"W_c\"][:, :128]\n",
    "    wia = cell[\"W_i\"][:, :128]\n",
    "    wfa = cell[\"W_f\"][:, :128]\n",
    "\n",
    "\n",
    "    grads[\"dLda_prev\"] = woa.T.dot(TdLdw_o) + wca.T.dot(TdLdw_c) + wia.T.dot(TdLdw_i) + wfa.T.dot(TdLdw_f)\n",
    "    grads[\"dLdc_prev\"] = (lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"]) * dldA_t)) * forward[\"F\"]\n",
    "\n",
    "\n",
    "    #not sure which side to transpose.\n",
    "    grads[\"dLdw_f\"] = np.atleast_2d(TdLdw_f).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_c\"] = np.atleast_2d(TdLdw_c).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_o\"] = np.atleast_2d(TdLdw_o).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_i\"] = np.atleast_2d(TdLdw_i).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_y\"] = (np.atleast_2d(TdLdw_y)).T.dot(np.atleast_2d(forward[\"A_t\"]))\n",
    "\n",
    "    grads[\"dLdb_f\"] = TdLdw_f\n",
    "    grads[\"dLdb_c\"] = TdLdw_c\n",
    "    grads[\"dLdb_o\"] = TdLdw_o\n",
    "    grads[\"dLdb_i\"] = TdLdw_i\n",
    "    grads[\"dLdb_y\"] = TdLdw_y\n",
    "\n",
    "\n",
    "    \n",
    "    loss = cross_entropy(forward[\"Yhat\"], Y)\n",
    "    # print(loss)\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(cell, X, input_size, hidden_size, lr, batch_size):\n",
    "   \n",
    "    \n",
    "\n",
    "    # for b in range(0, batch_size):\n",
    "\n",
    "    prevA = np.zeros(hidden_size)\n",
    "    prevC = np.zeros(hidden_size)\n",
    "\n",
    "    gradientTot = {}\n",
    "    lossTot = 0\n",
    "\n",
    "    allForwards = []\n",
    "    labels = []\n",
    "\n",
    "    lprimea = np.zeros(hidden_size)\n",
    "    lprimec = np.zeros(hidden_size)\n",
    "\n",
    "                            # np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_f\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_c\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_o\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_i\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_y\"] = np.zeros((10000,hidden_size),dtype=longdouble)\n",
    "    gradientTot[\"dLdb_f\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_c\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_o\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_i\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_y\"] = np.zeros(10000,dtype=longdouble)\n",
    "\n",
    "    X_b = X[0]\n",
    "\n",
    "    for i in range(1, len(X_b)-1): \n",
    "    \n",
    "        curData = X_b[0:i]\n",
    "        d = (X_b[0:i].astype(np.int64).tolist())\n",
    "\n",
    "        length = len(curData)\n",
    "\n",
    "        pad_length = input_size - length\n",
    "\n",
    "        if pad_length > 0:\n",
    "            curData = np.concatenate((curData,np.array([0] * pad_length)))\n",
    "        \n",
    "        forward = forward_pass(cell, prevA, prevC, curData)\n",
    "\n",
    "        allForwards.append(forward)\n",
    "\n",
    "        # next = np.random.choice(len(forward[\"Yhat\"]), p=forward[\"Yhat\"])\n",
    "    \n",
    "        prevA = forward[\"A_t\"]\n",
    "        prevC = forward[\"C_t\"]\n",
    "\n",
    "        label = np.zeros(10000, dtype=longdouble)\n",
    "       \n",
    "        label[int(X_b[i+1])] = 1.0\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # print(\"BACKPROP!!!!!!!!\\n\\n\\n\\n\")\n",
    "    for i in range(0, len(allForwards)):\n",
    "        # print(allForwards[i][\"Yhat\"].size)\n",
    "        grad, loss = gradient(allForwards[i], cell, labels[i], allForwards[i][\"Yhat\"], lprimea, lprimec)\n",
    "        lprimea = grad[\"dLda_prev\"]\n",
    "        lprimec = grad[\"dLdc_prev\"]\n",
    "\n",
    "        gradientTot[\"dLdw_f\"] += grad[\"dLdw_f\"]\n",
    "        gradientTot[\"dLdw_c\"] += grad[\"dLdw_c\"]\n",
    "        gradientTot[\"dLdw_o\"] += grad[\"dLdw_o\"]\n",
    "        gradientTot[\"dLdw_i\"] += grad[\"dLdw_i\"]\n",
    "        gradientTot[\"dLdw_y\"] += grad[\"dLdw_y\"]\n",
    "        \n",
    "        gradientTot[\"dLdb_f\"] += grad[\"dLdb_f\"]\n",
    "        gradientTot[\"dLdb_c\"] += grad[\"dLdb_c\"]\n",
    "        gradientTot[\"dLdb_o\"] += grad[\"dLdb_o\"]\n",
    "        gradientTot[\"dLdb_i\"] += grad[\"dLdb_i\"]\n",
    "        gradientTot[\"dLdb_y\"] += grad[\"dLdb_y\"]\n",
    "\n",
    "        # print(grad[\"dLdw_f\"], grad[\"dLdw_c\"], grad[\"dLdw_o\"], grad[\"dLdw_i\"])\n",
    "\n",
    "        lossTot += loss\n",
    "\n",
    "    cell[\"W_f\"] = cell[\"W_f\"] - gradientTot[\"dLdw_f\"] * lr\n",
    "    # print(gradientTot[\"dLdw_f\"])\n",
    "    cell[\"W_c\"] = cell[\"W_c\"] - gradientTot[\"dLdw_c\"] * lr\n",
    "    cell[\"W_o\"] = cell[\"W_o\"] - gradientTot[\"dLdw_o\"] * lr\n",
    "    cell[\"W_i\"] = cell[\"W_i\"] - gradientTot[\"dLdw_i\"] * lr\n",
    "    cell[\"W_y\"] = cell[\"W_y\"] - gradientTot[\"dLdw_y\"] * lr\n",
    "\n",
    "    # cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"]/f_length * lr\n",
    "    # cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"]/f_length * lr\n",
    "    # cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"]/f_length * lr\n",
    "    # cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"]/f_length * lr\n",
    "    # cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"]/f_length * lr\n",
    "    cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"] * lr\n",
    "    cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"] * lr\n",
    "    cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"] * lr\n",
    "    cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"] * lr\n",
    "    cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"] * lr\n",
    "\n",
    "    return lossTot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ad233117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(input_size, hidden_size, dataset, batch_size):\n",
    "\n",
    "    cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data in dataset:\n",
    "        # print(data)\n",
    "        loss = descent(cell, data, input_size, hidden_size, 0.001, batch_size)\n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d781b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1596.19it/s]\n",
      "100%|██████████| 3332/3332 [00:02<00:00, 1517.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "vocab_size = 10000\n",
    "\n",
    "hidden_size = 128\n",
    "input_size = vocab_size\n",
    "# + hidden_size\n",
    "\n",
    "dataset = load_emails(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "f2272f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_3892\\175777435.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.77189381e-05 9.90774001e-05 1.00632028e-04 ... 9.63852839e-05\n",
      " 1.02588473e-04 9.82675712e-05]\n",
      "[9.44961997e-05 9.64165526e-05 1.02821962e-04 ... 1.02119483e-04\n",
      " 1.03263192e-04 9.55917704e-05]\n",
      "[9.84303371e-05 9.65716920e-05 9.63237568e-05 ... 1.14025055e-04\n",
      " 1.10009974e-04 1.02876787e-04]\n",
      "[1.02477683e-04 9.76625507e-05 9.74235832e-05 ... 1.16975727e-04\n",
      " 1.10792189e-04 1.01206920e-04]\n",
      "[1.03228282e-04 9.82606946e-05 9.74637059e-05 ... 1.17925766e-04\n",
      " 1.11426555e-04 9.98534008e-05]\n",
      "[1.03225172e-04 9.83934999e-05 9.68869459e-05 ... 1.17279304e-04\n",
      " 1.10925598e-04 9.87789747e-05]\n",
      "[1.04087092e-04 9.97515154e-05 9.86972536e-05 ... 1.20807434e-04\n",
      " 1.07840725e-04 9.68281125e-05]\n",
      "[1.04310957e-04 1.01154836e-04 9.95912550e-05 ... 1.21566923e-04\n",
      " 1.09518972e-04 9.55706519e-05]\n",
      "[1.02878085e-04 1.04434266e-04 1.01811094e-04 ... 1.20312805e-04\n",
      " 1.09754699e-04 9.02008883e-05]\n",
      "[1.03724609e-04 1.03070518e-04 1.02842973e-04 ... 1.19231213e-04\n",
      " 1.08769059e-04 9.23026326e-05]\n",
      "[1.05324707e-04 9.55551364e-05 9.17816163e-05 ... 1.18910708e-04\n",
      " 1.05410682e-04 9.49260759e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[287], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m ex_email \u001b[38;5;241m=\u001b[39m dataset[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m finalCell \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[285], line 9\u001b[0m, in \u001b[0;36mtrain_LSTM\u001b[1;34m(input_size, hidden_size, dataset, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     11\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[284], line 68\u001b[0m, in \u001b[0;36mdescent\u001b[1;34m(cell, X, input_size, hidden_size, lr, batch_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# print(\"BACKPROP!!!!!!!!\\n\\n\\n\\n\")\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(allForwards)):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# print(allForwards[i][\"Yhat\"].size)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     grad, loss \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallForwards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallForwards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYhat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     lprimea \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLda_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     70\u001b[0m     lprimec \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdc_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[283], line 39\u001b[0m, in \u001b[0;36mgradient\u001b[1;34m(forward, cell, X, Y, lprimea, lprimec)\u001b[0m\n\u001b[0;32m     37\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_o\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_o)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m     38\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_i\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_i)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m---> 39\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_y\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTdLdw_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA_t\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdb_f\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TdLdw_f\n\u001b[0;32m     42\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdb_c\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TdLdw_c\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ex_email = dataset[random.randint(0, 50)]\n",
    "\n",
    "\n",
    "\n",
    "finalCell = train_LSTM(input_size, hidden_size, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "60268a3974607f958e98a9322dd45e6d52b600b97a1d9eef60ef15acbb670d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
