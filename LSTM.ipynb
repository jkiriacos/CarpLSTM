{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁sa', 'mp', 'le', '▁sentence', '▁to', '▁to', 'ken', 'ize', '.']\n",
      "[102, 22, 14, 1693, 2265, 421, 7627, 6, 6, 4296, 3055, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "text = \"This is a sample sentence to tokenize.\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens)\n",
    "print(sp.encode_as_ids(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sequence_length():\n",
    "    max_length = 0\n",
    "    prefix = './PhishingEmails/'  # Adjust this to your file path\n",
    "    \n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read()\n",
    "            email_dict = json.loads(jsonStr)\n",
    "            setupData = sp.encode_as_ids(\n",
    "                email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip()\n",
    "            )\n",
    "            max_length = max(max_length, len(setupData))\n",
    "    \n",
    "    return max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb41a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_emails(batch_size, input_size):\n",
    "\n",
    "    max_sequence_length = input_size\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "\n",
    "    count = 0 #just to limit dataset size at first\n",
    "    \n",
    "    prefix = './PhishingEmails/' #change this to the prefile thing such as './celebA'\n",
    "\n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "\n",
    "        # if count >= 200: #comment this out to get full data\n",
    "        #     break\n",
    "        \n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read() #json file as a string\n",
    "            email_dict = json.loads(jsonStr) #converts to dictionary\n",
    "        \n",
    "        setupData = sp.encode_as_ids(email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip())\n",
    "        pad_length = max_sequence_length - len(setupData)\n",
    "\n",
    "        if pad_length > 0:\n",
    "            setupData += [-1] * pad_length\n",
    "\n",
    "\n",
    "        if batch_counter < batch_size:\n",
    "            batch.append(setupData)\n",
    "            batch_counter += 1\n",
    "        else:\n",
    "            dataset.append(batch)\n",
    "            batch = []\n",
    "            batch_counter = 0\n",
    "       \n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize helpful functions for math\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "def tanh_derivative(x:np.ndarray):\n",
    "    return 1-np.square(tanh(x))\n",
    "\n",
    "def softmax(x: np.ndarray):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    loss = 0\n",
    "     \n",
    "    for i in range(len(yhat)):\n",
    "\n",
    "        loss = loss + (-1 * y[i]*np.log(yhat[i]))\n",
    " \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes the weights of the network\n",
    "def initialize_cell(input_size, hidden_size):\n",
    "\n",
    "    print(input_size)\n",
    "\n",
    "    cell = {}\n",
    "\n",
    "    # W = np.random.normal(0, 1, (hidden_size, input_size))  # Dimensions hidden_size x input_size\n",
    "    # U = np.random.normal(0, 1, (hidden_size, hidden_size))  # Dimensions hidden_size x hidden_size\n",
    "\n",
    "    # # Check the dimensions\n",
    "    # print(\"W shape:\", W.shape)  # Should print (256, 28756)\n",
    "    # print(\"U shape:\", U.shape)  # Should print (256, 256)\n",
    "\n",
    "    # # Vertically stack W and U\n",
    "    # input_data = np.vstack((W, U))\n",
    "    # print(\"Stacked input_data shape:\", input_data.shape)  # Should print (512, 28756)\n",
    "    print(input_size,\"-----\")\n",
    "\n",
    "    cell[\"W_i\"] = np.hstack((np.random.normal(0, 1, (hidden_size, hidden_size)), np.random.normal(0, 1, (hidden_size, input_size)))) #input gate weights\n",
    "    cell[\"W_f\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size)))) #forget gate weights\n",
    "    cell[\"W_c\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size)))) #candidate gate weights\n",
    "    cell[\"W_o\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size)))) #output gate weights\n",
    "    cell[\"W_y\"] = (np.random.normal(0,1,(hidden_size,hidden_size)))#final gate weights\n",
    "\n",
    "    #not sure if the biases need to be 3d...\n",
    "    cell[\"b_i\"] = np.zeros(hidden_size) #input gate biases\n",
    "    cell[\"b_f\"] = np.zeros(hidden_size) #forget gate biases\n",
    "    cell[\"b_c\"] = np.zeros(hidden_size) #candidate gate biases\n",
    "    cell[\"b_o\"] = np.zeros(hidden_size) #output gate biases\n",
    "    cell[\"b_y\"] = np.zeros(hidden_size) #final gate biases\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass of all gates\n",
    "def forward_pass(cell, prevA, prevC, X):\n",
    "\n",
    "    # print(X, \"/n______-\")\n",
    "    \n",
    "    input = np.hstack((prevA, X))\n",
    "\n",
    "    forward = {}\n",
    "\n",
    "    forward[\"F\"] = sigmoid(cell[\"W_f\"].dot(input) + cell[\"b_i\"])\n",
    "    \n",
    "    forward[\"C\"] = tanh(cell[\"W_c\"].dot(input) + cell[\"b_c\"])\n",
    "\n",
    "    forward[\"I\"] = sigmoid(cell[\"W_i\"].dot(input) + cell[\"b_i\"])\n",
    "\n",
    "    forward[\"O\"] = sigmoid(cell[\"W_o\"].dot(input) + cell[\"b_o\"])\n",
    "\n",
    "\n",
    "    forward[\"prevA\"] = prevA\n",
    "    forward[\"prevC\"] = prevC\n",
    "    forward[\"C_t\"] = (forward[\"prevC\"] * forward[\"F\"]) + (forward[\"I\"] * forward[\"C\"])\n",
    "    forward[\"A_t\"] = forward[\"O\"] * tanh(forward[\"C_t\"])\n",
    "\n",
    "    forward[\"Z_t\"] = cell[\"W_y\"].dot(forward[\"C_t\"] * forward[\"O\"]) + cell[\"b_y\"]\n",
    "    forward[\"Yhat\"] = softmax(forward[\"Z_t\"])\n",
    "\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(forward, cell, X, Y, lprimea, lprimec):\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    print(\"\\nLELELELEL\")\n",
    "    input = np.hstack((forward[\"prevA\"], X))\n",
    "\n",
    "    dldA_t = np.transpose(cell[\"W_y\"]).dot(forward[\"Yhat\"]-Y) + lprimea\n",
    "    dldC_t = lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"])) * dldA_t \n",
    "\n",
    "    TdLdw_f = (dldC_t * forward[\"prevC\"] * forward[\"F\"]*(1-forward[\"F\"])) \n",
    "    TdLdw_c = (dldC_t * forward[\"I\"])\n",
    "    TdLdw_o = (dldA_t * tanh(forward[\"C_t\"]) * forward[\"O\"] * (1-forward[\"O\"]))\n",
    "    TdLdw_i = (dldC_t * forward[\"C\"] * forward[\"I\"] * (1-forward[\"I\"]))\n",
    "\n",
    "    print(np.transpose(cell[\"W_y\"]).size, \" wyT\")\n",
    "    print((forward[\"Yhat\"]-Y).size, \" error\")\n",
    "    print(dldC_t.size, \"c_t\")\n",
    "    print(forward[\"prevC\"].size, \"prevC\")\n",
    "    print(forward[\"F\"].size, \" F\")\n",
    "    print(TdLdw_f.size, \" Tdldwf\")\n",
    "    print(input.size, \" input\")\n",
    "\n",
    "    # np.atleast2d(a).T\n",
    "\n",
    "    woa = cell[\"W_o\"][:, :128]\n",
    "    wca = cell[\"W_c\"][:, :128]\n",
    "    wia = cell[\"W_i\"][:, :128]\n",
    "    wfa = cell[\"W_f\"][:, :128]\n",
    "\n",
    "    print(woa.size, \" woa\")\n",
    "\n",
    "    grads[\"dLda_prev\"] = woa.T * TdLdw_o + wca.T * TdLdw_c + wia.T * TdLdw_i + wfa.T * TdLdw_f\n",
    "    grads[\"dLdc_prev\"] = (lprimec + forward[\"O\"] * 1-np.square(tanh(forward[\"C_t\"])) * dldA_t) * forward[\"F\"]\n",
    "\n",
    "    print(grads[\"dLda_prev\"].size, \" dlda\")\n",
    "    print(grads[\"dLdc_prev\"].size, \" dldc\")\n",
    "\n",
    "    #not sure which side to transpose.\n",
    "    grads[\"dLdw_f\"] = np.atleast_2d(TdLdw_f).dot(np.atleast_2d(input).T)\n",
    "    grads[\"dLdw_c\"] = np.atleast_2d(TdLdw_c).dot(np.atleast_2d(input).T)\n",
    "    grads[\"dLdw_o\"] = np.atleast_2d(TdLdw_o).dot(np.atleast_2d(input).T)\n",
    "    grads[\"dLdw_i\"] = np.atleast_2d(TdLdw_i).dot(np.atleast_2d(input).T)\n",
    "    grads[\"dLdw_y\"] = (forward[\"Yhat\"] - Y).dot(np.transpose(forward[\"A_t\"]))\n",
    "\n",
    "    grads[\"dLdb_f\"] = 1\n",
    "    grads[\"dLdb_c\"] = 1\n",
    "    grads[\"dLdb_o\"] = 1\n",
    "    grads[\"dLdb_i\"] = 1\n",
    "    grads[\"dLdb_y\"] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    loss = cross_entropy(forward[\"Yhat\"], Y)\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(cell, X, input_size, hidden_size, lr, batch_size):\n",
    "   \n",
    "    \n",
    "\n",
    "    for b in range(0, batch_size):\n",
    "\n",
    "        prevA = np.zeros(hidden_size)\n",
    "        prevC = np.zeros(hidden_size)\n",
    "\n",
    "        gradientTot = {}\n",
    "        lossTot = 0\n",
    "\n",
    "        allForwards = []\n",
    "        labels = []\n",
    "\n",
    "        lprimea = np.zeros(hidden_size)\n",
    "        lprimec = np.zeros(hidden_size)\n",
    "\n",
    "        gradientTot[\"dLdw_f\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "        gradientTot[\"dLdw_c\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "        gradientTot[\"dLdw_o\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "        gradientTot[\"dLdw_i\"] = np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "        gradientTot[\"dLdw_y\"] = (np.random.normal(0,1,(hidden_size,hidden_size)))\n",
    "        gradientTot[\"dLdb_f\"] = np.zeros(hidden_size)\n",
    "        gradientTot[\"dLdb_c\"] = np.zeros(hidden_size)\n",
    "        gradientTot[\"dLdb_o\"] = np.zeros(hidden_size)\n",
    "        gradientTot[\"dLdb_i\"] = np.zeros(hidden_size)\n",
    "        gradientTot[\"dLdb_y\"] = np.zeros(hidden_size)\n",
    "\n",
    "        X_b = X[b]\n",
    "\n",
    "        for i in range(1, len(X_b)-1):  #tqdm will create a loading bar for your loop\n",
    "\n",
    "            curData = X_b[0:i]\n",
    "\n",
    "            length = len(curData)\n",
    "\n",
    "            pad_length = input_size - length\n",
    "\n",
    "            if pad_length > 0:\n",
    "                curData = np.concatenate((curData,np.array([0] * pad_length)))\n",
    "            \n",
    "            forward = forward_pass(cell, prevA, prevC, curData)\n",
    "\n",
    "            allForwards.append(forward)\n",
    "\n",
    "            prevA = forward[\"A_t\"]\n",
    "            prevC = forward[\"C_t\"]\n",
    "\n",
    "            curData[length] = X_b[i+1]\n",
    "\n",
    "            labels.append(curData)\n",
    "\n",
    "        print(\"BACKTIME\")\n",
    "        for i in range(0, len(allForwards)):\n",
    "\n",
    "            grad, loss = gradient(allForwards[i], cell, labels[i], allForwards[i][\"Yhat\"], lprimea, lprimec)\n",
    "            lprimea = grad[\"dLda_prev\"]\n",
    "            lprimec = grad[\"dLdc_prev\"]\n",
    "\n",
    "            gradientTot[\"dLdw_f\"] += grad[\"dLdw_f\"]\n",
    "            gradientTot[\"dLdw_c\"] += grad[\"dLdw_c\"]\n",
    "            gradientTot[\"dLdw_o\"] += grad[\"dLdw_o\"]\n",
    "            gradientTot[\"dLdw_i\"] += grad[\"dLdw_i\"]\n",
    "            gradientTot[\"dLdw_y\"] += grad[\"dLdw_y\"]\n",
    "            \n",
    "            gradientTot[\"dLdb_f\"] += grad[\"dLdb_f\"]\n",
    "            gradientTot[\"dLdb_c\"] += grad[\"dLdb_c\"]\n",
    "            gradientTot[\"dLdb_o\"] += grad[\"dLdb_o\"]\n",
    "            gradientTot[\"dLdb_i\"] += grad[\"dLdb_i\"]\n",
    "            gradientTot[\"dLdb_y\"] += grad[\"dLdb_y\"]\n",
    "\n",
    "            lossTot += loss\n",
    "        \n",
    "\n",
    "        cell[\"W_f\"] = cell[\"W_f\"] - gradientTot[\"dLdw_f\"] * lr\n",
    "        cell[\"W_c\"] = cell[\"W_c\"] - gradientTot[\"dLdw_c\"] * lr\n",
    "        cell[\"W_o\"] = cell[\"W_o\"] - gradientTot[\"dLdw_o\"] * lr\n",
    "        cell[\"W_i\"] = cell[\"W_i\"] - gradientTot[\"dLdw_i\"] * lr\n",
    "        cell[\"W_y\"] = cell[\"W_y\"] - gradientTot[\"dLdw_y\"] * lr\n",
    "\n",
    "        cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"] * lr\n",
    "        cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"] * lr\n",
    "        cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"] * lr\n",
    "        cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"] * lr\n",
    "        cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"] * lr\n",
    "\n",
    "    return lossTot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad233117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(input_size, hidden_size, dataset, batch_size):\n",
    "\n",
    "    cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data in dataset:\n",
    "        loss = descent(cell, data, input_size, hidden_size, 0.0001, batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d781b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:03<00:00, 863.34it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 16\n",
    "vocab_size = 28500\n",
    "\n",
    "hidden_size = 128\n",
    "input_size = hidden_size + 28500\n",
    "\n",
    "dataset = load_emails(batch_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2272f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28628\n",
      "28628 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_15068\\1941146008.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_15068\\1941146008.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_15068\\1941146008.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKTIME\n",
      "\n",
      "LELELELEL\n",
      "16384  wyT\n",
      "128  error\n",
      "128 c_t\n",
      "128 prevC\n",
      "128  F\n",
      "128  Tdldwf\n",
      "28756  input\n",
      "16384  woa\n",
      "16384  dlda\n",
      "128  dldc\n",
      "\n",
      "LELELELEL\n",
      "16384  wyT\n",
      "128  error\n",
      "16384 c_t\n",
      "128 prevC\n",
      "128  F\n",
      "16384  Tdldwf\n",
      "28756  input\n",
      "16384  woa\n",
      "16384  dlda\n",
      "16384  dldc\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (128,128) and (1,28756) not aligned: 128 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m ex_email \u001b[38;5;241m=\u001b[39m dataset[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m finalCell \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mtrain_LSTM\u001b[1;34m(input_size, hidden_size, dataset, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m----> 8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell\n",
      "Cell \u001b[1;32mIn[14], line 57\u001b[0m, in \u001b[0;36mdescent\u001b[1;34m(cell, X, input_size, hidden_size, lr, batch_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBACKTIME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(allForwards)):\n\u001b[1;32m---> 57\u001b[0m     grad, loss \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallForwards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallForwards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYhat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     lprimea \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLda_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     59\u001b[0m     lprimec \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdc_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[13], line 39\u001b[0m, in \u001b[0;36mgradient\u001b[1;34m(forward, cell, X, Y, lprimea, lprimec)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLda_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dlda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdc_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dldc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_f\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTdLdw_f\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_c\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_c)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m))\n\u001b[0;32m     41\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_o\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_o)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (128,128) and (1,28756) not aligned: 128 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "ex_email = dataset[random.randint(0, 50)]\n",
    "\n",
    "\n",
    "\n",
    "finalCell = train_LSTM(input_size, hidden_size, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "60268a3974607f958e98a9322dd45e6d52b600b97a1d9eef60ef15acbb670d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
