{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "5da06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁sa', 'mp', 'le', '▁sentence', '▁to', '▁to', 'ken', 'ize', '.']\n",
      "[102, 22, 14, 1693, 2265, 421, 7627, 6, 6, 4296, 3055, 3]\n",
      "This is a sample sentence to tokenize.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from numpy import longdouble\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "text = \"This is a sample sentence to tokenize.\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens)\n",
    "print(sp.encode_as_ids(text))\n",
    "print(sp.decode(sp.encode_as_ids(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "783e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sequence_length():\n",
    "    maxlen = 0\n",
    "    prefix = './PhishingEmails/'  # Adjust this to your file path\n",
    "    \n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read()\n",
    "            email_dict = json.loads(jsonStr)\n",
    "            setupData = sp.encode_as_ids(\n",
    "                email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip()\n",
    "            )\n",
    "\n",
    "            if(len(setupData) < 801):\n",
    "                maxlen = max(maxlen, len(setupData))\n",
    "\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "edb41a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(batch_size, windows_size):\n",
    "\n",
    "    max_sequence_length = find_max_sequence_length()\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "    ctr = 0\n",
    "    \n",
    "    prefix = './PhishingEmails/' #change this to the prefile thing such as './celebA'\n",
    "\n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        \n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read() #json file as a string\n",
    "            email_dict = json.loads(jsonStr) #converts to dictionary\n",
    "        \n",
    "        setupData = sp.encode_as_ids(email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip())\n",
    "        pad_length = max_sequence_length - len(setupData)\n",
    "\n",
    "        if(len(setupData) <= max_sequence_length):\n",
    "            # print(setupData,\"\\n\\n\\n\")\n",
    "            ctr+=1\n",
    "\n",
    "            for i in range(windows_size, len(setupData) - windows_size - 1):\n",
    "\n",
    "                if batch_counter < batch_size:\n",
    "                    batch.append(np.array(setupData[i:i+windows_size],dtype=longdouble))\n",
    "                    batch_counter += 1\n",
    "                else:\n",
    "                    dataset.append(batch)\n",
    "                    batch = []\n",
    "                    batch_counter = 0\n",
    "\n",
    "    dataset = np.array(dataset)\n",
    "    # labels = np.array(labels)\n",
    "\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0e301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize helpful functions for math\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "def tanh_derivative(x:np.ndarray):\n",
    "    return 1-np.square(tanh(x))\n",
    "\n",
    "def softmax(x: np.ndarray):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy(yhat, y, epsilon=1e-10):\n",
    "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)  # Clip yhat to avoid zeros\n",
    "    return -np.sum(y * np.log(yhat_clipped))\n",
    "\n",
    "def initWeights(input_size, output_size):\n",
    "    return np.random.uniform(-1, 1, (input_size, output_size)).astype(longdouble) * np.sqrt(6 / (input_size + output_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes the weights of the network\n",
    "def initialize_cell(input_size, hidden_size):\n",
    "    \n",
    "\n",
    "    cell = {}\n",
    "\n",
    "    cell[\"W_i\"] = np.vstack((initWeights(hidden_size, hidden_size), initWeights(input_size, hidden_size))) #input gate weights\n",
    "    cell[\"W_f\"] = np.vstack((initWeights(hidden_size, hidden_size), initWeights(input_size, hidden_size))) #forget gate weights\n",
    "    cell[\"W_c\"] = np.vstack((initWeights(hidden_size, hidden_size), initWeights(input_size, hidden_size))) #candidate gate weights\n",
    "    cell[\"W_o\"] = np.vstack((initWeights(hidden_size, hidden_size), initWeights(input_size, hidden_size))) #output gate weights\n",
    "    cell[\"W_y\"] = initWeights(hidden_size, 10000)#final gate weights\n",
    "\n",
    "    #not sure if the biases need to be 3d...\n",
    "    cell[\"b_i\"] = np.zeros(hidden_size,dtype=longdouble) #input gate biases\n",
    "    cell[\"b_f\"] = np.zeros(hidden_size,dtype=longdouble) #forget gate biases\n",
    "    cell[\"b_c\"] = np.zeros(hidden_size,dtype=longdouble) #candidate gate biases\n",
    "    cell[\"b_o\"] = np.zeros(hidden_size,dtype=longdouble) #output gate biases\n",
    "    cell[\"b_y\"] = np.zeros(10000) #final gate biases\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass of all gates\n",
    "def forward_pass(cell, prevA, prevC, X):\n",
    "\n",
    "    # print(X, \"/n______-\")\n",
    "    \n",
    "    # print(X)\n",
    "    input = np.hstack((prevA, X))\n",
    "   \n",
    "\n",
    "    forward = {}\n",
    "    # print(cell[\"W_f\"])\n",
    "\n",
    "    forward[\"F\"] = sigmoid(input.dot(cell[\"W_f\"]) + cell[\"b_f\"])\n",
    "\n",
    "    forward[\"_c\"] = input.dot(cell[\"W_c\"]) + cell[\"b_c\"]\n",
    "    \n",
    "    forward[\"C\"] = tanh(forward[\"_c\"])\n",
    "\n",
    "    forward[\"I\"] = sigmoid(input.dot(cell[\"W_i\"]) + cell[\"b_i\"])\n",
    "\n",
    "    forward[\"O\"] = sigmoid(input.dot(cell[\"W_o\"]) + cell[\"b_o\"])\n",
    "\n",
    "\n",
    "    forward[\"prevA\"] = prevA\n",
    "    forward[\"prevC\"] = prevC\n",
    "    forward[\"C_t\"] = (forward[\"prevC\"] * forward[\"F\"]) + (forward[\"I\"] * forward[\"C\"])\n",
    "    forward[\"A_t\"] = forward[\"O\"] * tanh(forward[\"C_t\"])\n",
    "\n",
    "    forward[\"Z_t\"] = forward[\"A_t\"].dot(cell[\"W_y\"]) \n",
    "    # + cell[\"b_y\"]\n",
    "    \n",
    "    forward[\"Yhat\"] = softmax(forward[\"Z_t\"])\n",
    "    # \n",
    "    # print(forward[\"Yhat\"].size)\n",
    "    # print(forward[\"Yhat\"], \"  Yhat\")\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(forward, cell, X, Y, lprimea, lprimec, hidden_size):\n",
    "    # print(\"HELLLOOOOOOOOO\")\n",
    "    grads = {}\n",
    "\n",
    "    # print(\"BackProp\")\n",
    "    input = np.hstack((forward[\"prevA\"], X))\n",
    "    # print((forward[\"Yhat\"]-Y).size, \"yhat-y\")\n",
    "    # print((np.transpose(cell[\"W_y\"]).size))\n",
    "    # print(lprimea.size)\n",
    "    dldA_t = (forward[\"Yhat\"]-Y).dot(np.transpose(cell[\"W_y\"])) + lprimea\n",
    "    \n",
    "    dldC_t = lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"])) * dldA_t \n",
    "    # print(forward[\"Yhat\"]-Y)\n",
    "\n",
    "    TdLdw_f = (dldC_t * forward[\"prevC\"] * forward[\"F\"]*(1-forward[\"F\"])) \n",
    "    # TdLdw_c = (dldC_t * forward[\"I\"])\n",
    "    TdLdw_c = (dldC_t * forward[\"I\"]*tanh_derivative(forward[\"_c\"]))\n",
    "    TdLdw_o = (dldA_t * tanh(forward[\"C_t\"]) * forward[\"O\"] * (1-forward[\"O\"]))\n",
    "    TdLdw_i = (dldC_t * forward[\"C\"] * forward[\"I\"] * (1-forward[\"I\"]))\n",
    "    TdLdw_y = (forward[\"Yhat\"] - Y)\n",
    "\n",
    "    \n",
    "\n",
    "    # np.atleast2d(a).T\n",
    "\n",
    "    woa = cell[\"W_o\"][:hidden_size, :]\n",
    "    wca = cell[\"W_c\"][:hidden_size, :]\n",
    "    wia = cell[\"W_i\"][:hidden_size, :]\n",
    "    wfa = cell[\"W_f\"][:hidden_size, :]\n",
    "\n",
    "\n",
    "    # print(TdLdw_o.size)\n",
    "    # print(woa.size)\n",
    "\n",
    "    grads[\"dLda_prev\"] = TdLdw_o.dot(woa.T) + TdLdw_c.dot(wca.T) + TdLdw_i.dot(wia.T) + TdLdw_f.dot(wfa.T)\n",
    "    grads[\"dLdc_prev\"] = (lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"]) * dldA_t)) * forward[\"F\"]\n",
    "\n",
    "\n",
    "    #not sure which side to transpose.\n",
    "    grads[\"dLdw_f\"] = np.atleast_2d(input).T.dot(np.atleast_2d(TdLdw_f))\n",
    "    grads[\"dLdw_c\"] = np.atleast_2d(input).T.dot(np.atleast_2d(TdLdw_c))\n",
    "    grads[\"dLdw_o\"] = np.atleast_2d(input).T.dot(np.atleast_2d(TdLdw_o))\n",
    "    grads[\"dLdw_i\"] = np.atleast_2d(input).T.dot(np.atleast_2d(TdLdw_i))\n",
    "    grads[\"dLdw_y\"] = np.atleast_2d(np.atleast_2d(forward[\"A_t\"])).T.dot(np.atleast_2d(TdLdw_y))\n",
    "\n",
    "    grads[\"dLdb_f\"] = TdLdw_f.sum(axis=0)\n",
    "    grads[\"dLdb_c\"] = TdLdw_c.sum(axis=0)\n",
    "    grads[\"dLdb_o\"] = TdLdw_o.sum(axis=0)\n",
    "    grads[\"dLdb_i\"] = TdLdw_i.sum(axis=0)\n",
    "    grads[\"dLdb_y\"] = TdLdw_y.sum(axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "    loss = cross_entropy(forward[\"Yhat\"], Y)\n",
    "    # print(loss)\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descent(cell, X, input_size, hidden_size, batch_size, lr):\n",
    "\n",
    "    # for b in range(0, batch_size):\n",
    "\n",
    "    prevA = np.zeros((batch_size, hidden_size))\n",
    "    prevC = np.zeros((batch_size, hidden_size))\n",
    "\n",
    "    gradientTot = {}\n",
    "    lossTot = 0\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    inputs = []\n",
    "\n",
    "    allForwards = []\n",
    "\n",
    "    lprimea = np.zeros((batch_size, hidden_size))\n",
    "    lprimec = np.zeros((batch_size, hidden_size))\n",
    "\n",
    "    gradientTot[\"dLdw_f\"] = np.vstack((np.zeros((hidden_size,hidden_size)), np.zeros((input_size, hidden_size))))\n",
    "    gradientTot[\"dLdw_c\"] = np.vstack((np.zeros((hidden_size,hidden_size)), np.zeros((input_size, hidden_size))))\n",
    "    gradientTot[\"dLdw_o\"] = np.vstack((np.zeros((hidden_size,hidden_size)), np.zeros((input_size, hidden_size))))\n",
    "    gradientTot[\"dLdw_i\"] = np.vstack((np.zeros((hidden_size,hidden_size)), np.zeros((input_size, hidden_size))))\n",
    "    gradientTot[\"dLdw_y\"] = np.zeros((hidden_size, 10000),dtype=longdouble)\n",
    "    \n",
    "    gradientTot[\"dLdb_f\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_c\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_o\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_i\"] = np.zeros(hidden_size,dtype=longdouble)\n",
    "    gradientTot[\"dLdb_y\"] = np.zeros(10000,dtype=longdouble)\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(1, input_size-1):\n",
    "\n",
    "        blabel = []\n",
    "\n",
    "        input = np.copy(X)\n",
    "\n",
    "        for minibatch in input:\n",
    "            for token in range(i, input_size):\n",
    "                minibatch[token] = 1\n",
    "\n",
    "        inputs.append(input)\n",
    "        # print(len(X))\n",
    "        for mini in X:\n",
    "            \n",
    "            label = np.zeros(10000, dtype=longdouble)\n",
    "            label[int(mini[i+1])] = longdouble(1)\n",
    "\n",
    "            blabel.append(label)\n",
    "            \n",
    "\n",
    "        forward = forward_pass(cell, prevA, prevC, input)\n",
    "\n",
    "        prevA = forward[\"A_t\"]\n",
    "        prevC = forward[\"C_t\"]\n",
    "    \n",
    "        labels.append(blabel)\n",
    "        allForwards.append(forward)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(0, len(allForwards)):\n",
    "     \n",
    "        grad, loss = gradient(allForwards[i], cell, inputs[i], labels[i], lprimea, lprimec, hidden_size)\n",
    "        lprimea = grad[\"dLda_prev\"]\n",
    "        lprimec = grad[\"dLdc_prev\"]\n",
    "\n",
    "        gradientTot[\"dLdw_f\"] += grad[\"dLdw_f\"]\n",
    "        gradientTot[\"dLdw_c\"] += grad[\"dLdw_c\"]\n",
    "        gradientTot[\"dLdw_o\"] += grad[\"dLdw_o\"]\n",
    "        gradientTot[\"dLdw_i\"] += grad[\"dLdw_i\"]\n",
    "        gradientTot[\"dLdw_y\"] += grad[\"dLdw_y\"]\n",
    "        \n",
    "        gradientTot[\"dLdb_f\"] += grad[\"dLdb_f\"]\n",
    "        gradientTot[\"dLdb_c\"] += grad[\"dLdb_c\"]\n",
    "        gradientTot[\"dLdb_o\"] += grad[\"dLdb_o\"]\n",
    "        gradientTot[\"dLdb_i\"] += grad[\"dLdb_i\"]\n",
    "        gradientTot[\"dLdb_y\"] += grad[\"dLdb_y\"]\n",
    "\n",
    "        lossTot += loss\n",
    "\n",
    "    cell[\"W_f\"] = cell[\"W_f\"] - gradientTot[\"dLdw_f\"] * lr\n",
    "    cell[\"W_c\"] = cell[\"W_c\"] - gradientTot[\"dLdw_c\"] * lr\n",
    "    cell[\"W_o\"] = cell[\"W_o\"] - gradientTot[\"dLdw_o\"] * lr\n",
    "    cell[\"W_i\"] = cell[\"W_i\"] - gradientTot[\"dLdw_i\"] * lr\n",
    "    cell[\"W_y\"] = cell[\"W_y\"] - gradientTot[\"dLdw_y\"] * lr\n",
    "\n",
    "    cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"] * lr\n",
    "    cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"] * lr\n",
    "    cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"] * lr\n",
    "    cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"] * lr\n",
    "    cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"] * lr\n",
    "\n",
    "    return lossTot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ad233117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(dataset, input_size, hidden_size, batch_size, lr):\n",
    "\n",
    "    cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for i in range(0,100):\n",
    "        # print(data)\n",
    "        loss = descent(cell, dataset[i], input_size, hidden_size, batch_size, lr)\n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return cell, losses\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# def train_LSTM(dataset, labels, input_size, hidden_size, batch_size, K):\n",
    "\n",
    "#     cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     kfold = KFold(K, shuffle = True)\n",
    "\n",
    "#     for _, (xtrain, xtest, ytrain, yvalid) in enumerate(kfold.split(dataset, labels)):\n",
    "\n",
    "\n",
    "#         # print(data)\n",
    "#         loss = descent(cell, xtrain, ytrain, input_size, hidden_size, batch_size, 0.001)\n",
    "#         print(loss)\n",
    "#         losses.append(loss)\n",
    "\n",
    "#     return cell, losses\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d781b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3332 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1412.90it/s]\n",
      "100%|██████████| 3332/3332 [00:21<00:00, 154.72it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 8\n",
    "input_size = 50\n",
    "# windows_size = 50\n",
    "\n",
    "hidden_size = 128\n",
    "# + hidden_size\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_emails(batch_size, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f2272f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_12292\\4162846258.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4331.448971487918\n",
      "4291.4089292889275\n",
      "4224.104624444064\n",
      "4139.680700524046\n",
      "4055.2773643485275\n",
      "3826.804541253261\n",
      "3689.7927222071316\n",
      "3706.9730301896448\n",
      "3632.870172427593\n",
      "3804.517057520633\n",
      "3580.3993989157325\n",
      "3666.0342753725204\n",
      "4128.917453472521\n",
      "4762.374937616031\n",
      "4654.860922297721\n",
      "4720.030698170162\n",
      "5107.8101075405075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[261], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ex_email \u001b[38;5;241m=\u001b[39m dataset[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m finalCell, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(losses)) \u001b[38;5;241m*\u001b[39m batch_size, losses)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining curve\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[260], line 9\u001b[0m, in \u001b[0;36mtrain_LSTM\u001b[1;34m(dataset, input_size, hidden_size, batch_size, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     11\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[259], line 69\u001b[0m, in \u001b[0;36mdescent\u001b[1;34m(cell, X, input_size, hidden_size, batch_size, lr)\u001b[0m\n\u001b[0;32m     60\u001b[0m     allForwards\u001b[38;5;241m.\u001b[39mappend(forward)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(allForwards)):\n\u001b[1;32m---> 69\u001b[0m     grad, loss \u001b[38;5;241m=\u001b[39m \u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallForwards\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlprimec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     lprimea \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLda_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     71\u001b[0m     lprimec \u001b[38;5;241m=\u001b[39m grad[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdc_prev\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[253], line 44\u001b[0m, in \u001b[0;36mgradient\u001b[1;34m(forward, cell, X, Y, lprimea, lprimec, hidden_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_o\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_o))\n\u001b[0;32m     43\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_i\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39matleast_2d(TdLdw_i))\n\u001b[1;32m---> 44\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdw_y\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA_t\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matleast_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTdLdw_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdb_f\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TdLdw_f\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     47\u001b[0m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdLdb_c\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TdLdw_c\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ex_email = dataset[random.randint(0, 50)]\n",
    "\n",
    "finalCell, losses = train_LSTM(dataset, input_size, hidden_size, batch_size, 0.005)\n",
    "\n",
    "plt.plot(np.arange(len(losses)) * batch_size, losses)\n",
    "plt.title(\"training curve\")\n",
    "plt.xlabel(\"number of emails trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "60268a3974607f958e98a9322dd45e6d52b600b97a1d9eef60ef15acbb670d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
