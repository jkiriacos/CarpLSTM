{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "5da06227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁This', '▁is', '▁a', '▁sa', 'mp', 'le', '▁sentence', '▁to', '▁to', 'ken', 'ize', '.']\n",
      "[102, 22, 14, 1693, 2265, 421, 7627, 6, 6, 4296, 3055, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from numpy import longdouble\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_model.model')\n",
    "\n",
    "text = \"This is a sample sentence to tokenize.\"\n",
    "tokens = sp.encode_as_pieces(text)\n",
    "print(tokens)\n",
    "print(sp.encode_as_ids(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "783e6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_sequence_length():\n",
    "    maxlen = 0\n",
    "    prefix = './PhishingEmails/'  # Adjust this to your file path\n",
    "    \n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read()\n",
    "            email_dict = json.loads(jsonStr)\n",
    "            setupData = sp.encode_as_ids(\n",
    "                email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip()\n",
    "            )\n",
    "\n",
    "            if(len(setupData) < 725):\n",
    "                maxlen = max(maxlen, len(setupData))\n",
    "\n",
    "    return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "09af4f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1549.05it/s]\n",
      "100%|██████████| 3332/3332 [00:01<00:00, 1684.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 4.920e+02,  4.300e+01,  3.710e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.920e+02,  1.464e+03,  6.897e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 3.415e+03,  1.515e+03,  4.510e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 1.460e+02,  1.700e+01,  2.490e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.905e+03,  4.920e+02,  6.800e+01, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.920e+02,  3.710e+02,  4.030e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]],\n",
       "\n",
       "       [[ 6.900e+01,  3.200e+02,  9.390e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 3.980e+03,  3.000e+00,  2.540e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 3.480e+02,  5.300e+01,  5.070e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 1.515e+03,  6.856e+03,  3.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.410e+02,  5.251e+03,  3.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 8.680e+02,  9.044e+03,  4.400e+01, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]],\n",
       "\n",
       "       [[ 3.600e+01,  1.950e+02,  3.903e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.269e+03,  4.300e+01,  1.960e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 5.221e+03,  7.043e+03,  1.810e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 3.050e+02,  4.613e+03,  1.986e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 9.000e+00,  8.690e+02,  1.000e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 5.500e+02,  1.631e+03,  3.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.249e+03,  1.504e+03,  3.820e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 1.300e+01,  3.000e+00,  2.540e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 2.320e+02,  3.420e+02,  1.850e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 1.536e+03,  4.920e+02,  9.592e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 8.860e+02,  3.000e+00,  9.430e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.920e+02,  4.300e+01,  3.280e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]],\n",
       "\n",
       "       [[ 1.300e+01,  8.188e+03,  2.400e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 1.569e+03,  2.809e+03,  3.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 1.515e+03,  3.000e+00,  6.800e+01, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 8.660e+02,  4.300e+01,  2.810e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 1.210e+03,  1.824e+03,  3.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 3.900e+01,  4.227e+03,  1.300e+01, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]],\n",
       "\n",
       "       [[ 2.434e+03,  3.000e+00,  2.434e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 2.347e+03,  2.054e+03,  1.290e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 3.919e+03,  7.000e+00,  4.000e+00, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        ...,\n",
       "        [ 1.569e+03,  1.248e+03,  6.820e+02, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 5.546e+03,  1.387e+03,  5.408e+03, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00],\n",
       "        [ 4.410e+02,  5.640e+02,  7.900e+01, ..., -1.000e+00,\n",
       "         -1.000e+00, -1.000e+00]]], dtype=float64)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_emails(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "edb41a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(batch_size):\n",
    "\n",
    "    max_sequence_length = find_max_sequence_length()\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "    ctr = 0\n",
    "    \n",
    "    prefix = './PhishingEmails/' #change this to the prefile thing such as './celebA'\n",
    "\n",
    "    for file in tqdm(os.listdir(prefix)):\n",
    "        \n",
    "        with open(prefix + file) as jsonFile:\n",
    "            jsonStr = jsonFile.read() #json file as a string\n",
    "            email_dict = json.loads(jsonStr) #converts to dictionary\n",
    "        \n",
    "        setupData = sp.encode_as_ids(email_dict['email_subject'].strip() + \". \" + email_dict['email_body'].strip())\n",
    "        pad_length = max_sequence_length - len(setupData)\n",
    "\n",
    "        if(len(setupData) <= max_sequence_length):\n",
    "            ctr+=1\n",
    "            if pad_length > 0:\n",
    "                setupData += [-1] * pad_length\n",
    "\n",
    "            if batch_counter < batch_size:\n",
    "                batch.append(setupData)\n",
    "                batch_counter += 1\n",
    "            else:\n",
    "                dataset.append(batch)\n",
    "                batch = []\n",
    "                batch_counter = 0\n",
    "    \n",
    "    \n",
    "    return np.array(dataset, dtype=longdouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "0e301245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize helpful functions for math\n",
    "def sigmoid(x: np.ndarray):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x: np.ndarray):\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "def tanh_derivative(x:np.ndarray):\n",
    "    return 1-np.square(tanh(x))\n",
    "\n",
    "def softmax(x: np.ndarray):\n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy(yhat, y, epsilon=1e-10):\n",
    "    yhat_clipped = np.clip(yhat, epsilon, 1 - epsilon)  # Clip yhat to avoid zeros\n",
    "    return -np.sum(y * np.log(yhat_clipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "18d1a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializes the weights of the network\n",
    "def initialize_cell(input_size, hidden_size):\n",
    "\n",
    "    print(input_size)\n",
    "\n",
    "    cell = {}\n",
    "\n",
    "    cell[\"W_i\"] = np.hstack((np.random.normal(0, 0.01, (hidden_size, hidden_size)), np.random.normal(0, 0.01, (hidden_size, input_size)))) #input gate weights\n",
    "    cell[\"W_f\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #forget gate weights\n",
    "    cell[\"W_c\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #candidate gate weights\n",
    "    cell[\"W_o\"] = np.hstack((np.random.normal(0,0.01,(hidden_size,hidden_size)), np.random.normal(0,0.01,(hidden_size,input_size)))) #output gate weights\n",
    "    cell[\"W_y\"] = (np.random.normal(0,1,(hidden_size,hidden_size)))#final gate weights\n",
    "\n",
    "    #not sure if the biases need to be 3d...\n",
    "    cell[\"b_i\"] = np.zeros(hidden_size) #input gate biases\n",
    "    cell[\"b_f\"] = np.zeros(hidden_size) #forget gate biases\n",
    "    cell[\"b_c\"] = np.zeros(hidden_size) #candidate gate biases\n",
    "    cell[\"b_o\"] = np.zeros(hidden_size) #output gate biases\n",
    "    cell[\"b_y\"] = np.zeros(hidden_size) #final gate biases\n",
    "\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "348ff4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass of all gates\n",
    "def forward_pass(cell, prevA, prevC, X):\n",
    "\n",
    "    # print(X, \"/n______-\")\n",
    "    \n",
    "    input = np.hstack((prevA, X))\n",
    "\n",
    "    forward = {}\n",
    "\n",
    "    forward[\"F\"] = sigmoid(cell[\"W_f\"].dot(input.T) + cell[\"b_i\"])\n",
    "    \n",
    "    forward[\"C\"] = tanh(cell[\"W_c\"].dot(input.T) + cell[\"b_c\"])\n",
    "\n",
    "    forward[\"I\"] = sigmoid(cell[\"W_i\"].dot(input.T) + cell[\"b_i\"])\n",
    "\n",
    "    forward[\"O\"] = sigmoid(cell[\"W_o\"].dot(input.T) + cell[\"b_o\"])\n",
    "\n",
    "\n",
    "    forward[\"prevA\"] = prevA\n",
    "    forward[\"prevC\"] = prevC\n",
    "    forward[\"C_t\"] = (forward[\"prevC\"] * forward[\"F\"]) + (forward[\"I\"] * forward[\"C\"])\n",
    "    forward[\"A_t\"] = forward[\"O\"] * tanh(forward[\"C_t\"])\n",
    "\n",
    "    forward[\"Z_t\"] = cell[\"W_y\"].dot(forward[\"C_t\"] * forward[\"O\"]) + cell[\"b_y\"]\n",
    "    # print(forward[\"Z_t\"], \"  Z\")\n",
    "    forward[\"Yhat\"] = softmax(forward[\"Z_t\"])\n",
    "\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "e7355acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(forward, cell, X, Y, lprimea, lprimec):\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    # print(\"BackProp\")\n",
    "    input = np.hstack((forward[\"prevA\"], X))\n",
    "\n",
    "    dldA_t = np.transpose(cell[\"W_y\"]).dot(forward[\"Yhat\"]-Y) + lprimea\n",
    "    dldC_t = lprimec + (forward[\"O\"] * tanh_derivative(forward[\"C_t\"])) * dldA_t \n",
    "\n",
    "    TdLdw_f = (dldC_t * forward[\"prevC\"] * forward[\"F\"]*(1-forward[\"F\"])) \n",
    "    TdLdw_c = (dldC_t * forward[\"I\"])\n",
    "    TdLdw_o = (dldA_t * tanh(forward[\"C_t\"]) * forward[\"O\"] * (1-forward[\"O\"]))\n",
    "    TdLdw_i = (dldC_t * forward[\"C\"] * forward[\"I\"] * (1-forward[\"I\"]))\n",
    "\n",
    "    # np.atleast2d(a).T\n",
    "\n",
    "    woa = cell[\"W_o\"][:, :128]\n",
    "    wca = cell[\"W_c\"][:, :128]\n",
    "    wia = cell[\"W_i\"][:, :128]\n",
    "    wfa = cell[\"W_f\"][:, :128]\n",
    "\n",
    "\n",
    "    grads[\"dLda_prev\"] = woa.T.dot(TdLdw_o) + wca.T.dot(TdLdw_c) + wia.T.dot(TdLdw_i) + wfa.T.dot(TdLdw_f)\n",
    "    grads[\"dLdc_prev\"] = (lprimec + forward[\"O\"] * 1-np.square(tanh(forward[\"C_t\"])) * dldA_t) * forward[\"F\"]\n",
    "\n",
    "\n",
    "    #not sure which side to transpose.\n",
    "    grads[\"dLdw_f\"] = np.atleast_2d(TdLdw_f).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_c\"] = np.atleast_2d(TdLdw_c).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_o\"] = np.atleast_2d(TdLdw_o).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_i\"] = np.atleast_2d(TdLdw_i).T.dot(np.atleast_2d(input))\n",
    "    grads[\"dLdw_y\"] = (forward[\"Yhat\"] - Y).T.dot(np.transpose(forward[\"A_t\"]))\n",
    "\n",
    "    grads[\"dLdb_f\"] = 1\n",
    "    grads[\"dLdb_c\"] = 1\n",
    "    grads[\"dLdb_o\"] = 1\n",
    "    grads[\"dLdb_i\"] = 1\n",
    "    grads[\"dLdb_y\"] = 1\n",
    "\n",
    "\n",
    "    \n",
    "    loss = cross_entropy(forward[\"Yhat\"], Y)\n",
    "    print(loss)\n",
    "\n",
    "    return grads, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "fe29d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(cell, X, input_size, hidden_size, lr, batch_size):\n",
    "   \n",
    "    \n",
    "\n",
    "    # for b in range(0, batch_size):\n",
    "\n",
    "    prevA = np.zeros(hidden_size)\n",
    "    prevC = np.zeros(hidden_size)\n",
    "\n",
    "    gradientTot = {}\n",
    "    lossTot = 0\n",
    "\n",
    "    allForwards = []\n",
    "    labels = []\n",
    "\n",
    "    lprimea = np.zeros(hidden_size)\n",
    "    lprimec = np.zeros(hidden_size)\n",
    "\n",
    "                            # np.hstack((np.random.normal(0,1,(hidden_size,hidden_size)), np.random.normal(0,1,(hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_f\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_c\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_o\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_i\"] = np.hstack((np.zeros((hidden_size,hidden_size)), np.zeros((hidden_size,input_size))))\n",
    "    gradientTot[\"dLdw_y\"] = np.zeros((hidden_size,hidden_size))\n",
    "    gradientTot[\"dLdb_f\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_c\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_o\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_i\"] = np.zeros(hidden_size)\n",
    "    gradientTot[\"dLdb_y\"] = np.zeros(hidden_size)\n",
    "\n",
    "    X_b = X[0]\n",
    "\n",
    "    for i in range(1, len(X_b)-1):  #tqdm will create a loading bar for your loop\n",
    "    \n",
    "        curData = X_b[0:i]\n",
    "\n",
    "        length = len(curData)\n",
    "\n",
    "        pad_length = input_size - length\n",
    "\n",
    "        if pad_length > 0:\n",
    "            curData = np.concatenate((curData,np.array([0] * pad_length)))\n",
    "        \n",
    "        forward = forward_pass(cell, prevA, prevC, curData)\n",
    "\n",
    "        allForwards.append(forward)\n",
    "\n",
    "        prevA = forward[\"A_t\"]\n",
    "        prevC = forward[\"C_t\"]\n",
    "\n",
    "        curData[length] = X_b[i+1]\n",
    "\n",
    "        labels.append(curData)\n",
    "\n",
    "\n",
    "    for i in range(0, len(allForwards)):\n",
    "      \n",
    "\n",
    "\n",
    "        grad, loss = gradient(allForwards[i], cell, labels[i], allForwards[i][\"Yhat\"], lprimea, lprimec)\n",
    "        lprimea = grad[\"dLda_prev\"]\n",
    "        lprimec = grad[\"dLdc_prev\"]\n",
    "\n",
    "        gradientTot[\"dLdw_f\"] += grad[\"dLdw_f\"]\n",
    "        gradientTot[\"dLdw_c\"] += grad[\"dLdw_c\"]\n",
    "        gradientTot[\"dLdw_o\"] += grad[\"dLdw_o\"]\n",
    "        gradientTot[\"dLdw_i\"] += grad[\"dLdw_i\"]\n",
    "        gradientTot[\"dLdw_y\"] += grad[\"dLdw_y\"]\n",
    "        \n",
    "        gradientTot[\"dLdb_f\"] += grad[\"dLdb_f\"]\n",
    "        gradientTot[\"dLdb_c\"] += grad[\"dLdb_c\"]\n",
    "        gradientTot[\"dLdb_o\"] += grad[\"dLdb_o\"]\n",
    "        gradientTot[\"dLdb_i\"] += grad[\"dLdb_i\"]\n",
    "        gradientTot[\"dLdb_y\"] += grad[\"dLdb_y\"]\n",
    "\n",
    "        # print(grad[\"dLdw_f\"], grad[\"dLdw_c\"], grad[\"dLdw_o\"], grad[\"dLdw_i\"])\n",
    "\n",
    "        lossTot += loss\n",
    "    \n",
    "\n",
    "    cell[\"W_f\"] = cell[\"W_f\"] - gradientTot[\"dLdw_f\"] * lr\n",
    "    cell[\"W_c\"] = cell[\"W_c\"] - gradientTot[\"dLdw_c\"] * lr\n",
    "    cell[\"W_o\"] = cell[\"W_o\"] - gradientTot[\"dLdw_o\"] * lr\n",
    "    cell[\"W_i\"] = cell[\"W_i\"] - gradientTot[\"dLdw_i\"] * lr\n",
    "    cell[\"W_y\"] = cell[\"W_y\"] - gradientTot[\"dLdw_y\"] * lr\n",
    "\n",
    "    cell[\"b_f\"] = cell[\"b_f\"] - gradientTot[\"dLdb_f\"] * lr\n",
    "    cell[\"b_c\"] = cell[\"b_c\"] - gradientTot[\"dLdb_c\"] * lr\n",
    "    cell[\"b_o\"] = cell[\"b_o\"] - gradientTot[\"dLdb_o\"] * lr\n",
    "    cell[\"b_i\"] = cell[\"b_i\"] - gradientTot[\"dLdb_i\"] * lr\n",
    "    cell[\"b_y\"] = cell[\"b_y\"] - gradientTot[\"dLdb_y\"] * lr\n",
    "\n",
    "    return lossTot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "ad233117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LSTM(input_size, hidden_size, dataset, batch_size):\n",
    "\n",
    "    cell = initialize_cell(input_size, hidden_size)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for data in dataset:\n",
    "        # print(data)\n",
    "        loss = descent(cell, data, input_size, hidden_size, 0.0000001, batch_size)\n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return cell\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "d781b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3332 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3332/3332 [00:02<00:00, 1651.86it/s]\n",
      "100%|██████████| 3332/3332 [00:02<00:00, 1645.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 1\n",
    "vocab_size = 724\n",
    "\n",
    "hidden_size = 128\n",
    "input_size = hidden_size + vocab_size\n",
    "\n",
    "dataset = load_emails(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "f2272f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-1*x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_13084\\1301727613.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  return (np.exp(x) - np.exp(-x))/(np.exp(x)+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20092977405015566\n",
      "0.0220876604587938\n",
      "0.8131610774118752\n",
      "0.7211402286284112\n",
      "0.20343015662455177\n",
      "0.19019996832393518\n",
      "0.004130198276535954\n",
      "4.911308633153806e-07\n",
      "1.9641052656038936e-05\n",
      "3.694345562886476e-05\n",
      "0.1250967651932679\n",
      "0.1579549349333038\n",
      "0.04485139423431001\n",
      "3.334123011966157e-05\n",
      "5.664463798390072e-06\n",
      "8.977254895553336e-07\n",
      "1.551897328471555e-07\n",
      "2.373103066498023e-08\n",
      "2.0723388087438764e-09\n",
      "1.0675555015722415e-10\n",
      "2.1168725597021174e-10\n",
      "2.0509292049383537e-09\n",
      "1.102358002685329e-08\n",
      "1.2720077002741202e-06\n",
      "0.00012017237046209046\n",
      "1.0203378834708382e-10\n",
      "1.9752037566202812e-09\n",
      "5.727030834537152e-07\n",
      "1.0798628575067246e-05\n",
      "0.005839818448530705\n",
      "0.5284824598384505\n",
      "0.022447437334876003\n",
      "0.0007421384753853961\n",
      "3.6347474387112053e-06\n",
      "0.3759146625363706\n",
      "0.022929345921633443\n",
      "0.2180318329908238\n",
      "0.4717974142180834\n",
      "1.1030487182906863\n",
      "0.9179201636478507\n",
      "0.8238183729585111\n",
      "0.0058264769834530565\n",
      "0.0022805834783848314\n",
      "0.0008814212564638296\n",
      "3.9816206261128376e-08\n",
      "1.3896305436135879e-06\n",
      "9.57446985407025e-06\n",
      "5.201416639121861e-05\n",
      "0.0002532704124474874\n",
      "0.5074826920010858\n",
      "0.0502875316365405\n",
      "0.00013979675381781774\n",
      "1.0000007405000837e-10\n",
      "1.0000000828399824e-10\n",
      "1.0000000827904104e-10\n",
      "1.0000000827903711e-10\n",
      "1.0000000827903711e-10\n",
      "1.000000082790371e-10\n",
      "1.001438792370352e-10\n",
      "1.0014679007635391e-10\n",
      "1.0006610680290979e-10\n",
      "1.0012186350732956e-10\n",
      "1.0017411358186148e-10\n",
      "1.1149760835079242e-06\n",
      "4.149290702355725e-05\n",
      "0.0005539462314422631\n",
      "0.005458051753281541\n",
      "0.18615261096607347\n",
      "0.6634827470795185\n",
      "0.08851349641575329\n",
      "0.004806074768500403\n",
      "0.00022244220554373275\n",
      "1.000561436471994e-10\n",
      "1.0000131934973771e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.0000003940129722e-10\n",
      "1.0000130849509865e-10\n",
      "1.0002544533358585e-10\n",
      "1.6163389585661425e-09\n",
      "3.7426238532763773e-07\n",
      "0.0014967897066219668\n",
      "9.874950950751299e-08\n",
      "1.0670159324063682e-10\n",
      "1.0000030144180915e-10\n",
      "1.0000000827923707e-10\n",
      "1.0000000827903748e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.0000003583800744e-10\n",
      "1.0000000828191729e-10\n",
      "1.0000001006668772e-10\n",
      "1.0000000830361044e-10\n",
      "1.0000000827937771e-10\n",
      "1.0000000827904178e-10\n",
      "1.0000000827903716e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.0000000827934336e-10\n",
      "1.0000000827904317e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "1.000000082790371e-10\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "0.0001344783150191611\n",
      "nan\n",
      "1.0008074377930251e-10\n",
      "1.0125719822486143e-10\n",
      "1.0010083813535932e-10\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[416], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m ex_email \u001b[38;5;241m=\u001b[39m dataset[random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m finalCell \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[414], line 9\u001b[0m, in \u001b[0;36mtrain_LSTM\u001b[1;34m(input_size, hidden_size, dataset, batch_size)\u001b[0m\n\u001b[0;32m      5\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(data)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mdescent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m     11\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[1;32mIn[413], line 44\u001b[0m, in \u001b[0;36mdescent\u001b[1;34m(cell, X, input_size, hidden_size, lr, batch_size)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pad_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     42\u001b[0m     curData \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((curData,np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m pad_length)))\n\u001b[1;32m---> 44\u001b[0m forward \u001b[38;5;241m=\u001b[39m \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m allForwards\u001b[38;5;241m.\u001b[39mappend(forward)\n\u001b[0;32m     48\u001b[0m prevA \u001b[38;5;241m=\u001b[39m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[411], line 16\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(cell, prevA, prevC, X)\u001b[0m\n\u001b[0;32m     12\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tanh(cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_c\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_c\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     14\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sigmoid(cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 16\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sigmoid(\u001b[43mcell\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mW_o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m cell[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_o\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     19\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevA\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prevA\n\u001b[0;32m     20\u001b[0m forward[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevC\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prevC\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "ex_email = dataset[random.randint(0, 50)]\n",
    "\n",
    "\n",
    "\n",
    "finalCell = train_LSTM(input_size, hidden_size, dataset, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "60268a3974607f958e98a9322dd45e6d52b600b97a1d9eef60ef15acbb670d09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
